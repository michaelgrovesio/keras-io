{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Object Detection with Mike\n",
    "\n",
    "Author: Michael<br>\n",
"Date created: 2023/04/22<br>\n",
"Last modified: 2023/04/22<br>\n",
"Description: Use KerasCV to augment images with CutMix, MixUp, RandAugment, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
  "KerasCV is a tool that helps solve problems related to detecting objects. \n",
"It has many features like tools to make the data better, \n",
"ways to measure how well the tool is working, and pre-made models to use. \n",
"With KerasCV, you can train your own tool to detect objects too! \n",
"Let's try out KerasCV's object detection tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!!pip install --upgrade git+https://github.com/keras-team/keras-cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "import keras_cv\n",
    "import numpy as np\n",
    "from keras_cv import bounding_box\n",
    "import os\n",
    "import resource\n",
    "from keras_cv import visualization\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Object detection introduction\n",
    "\n",
    "Object detection is the process of identifying, classifying,\n",
    "and localizing objects within a given image.  Typically, your inputs are\n",
    "images, and your labels are bounding boxes with optional class\n",
    "labels.\n",
    "Object detection can be thought of as an extension of classification, however\n",
    "instead of one class label for the image, you must detect and localize and\n",
    "arbitrary number of classes.\n",
    "\n",
    "**For example:**\n",
    "\n",
    "<img width=\"300\" src=\"https://i.imgur.com/QTvxIZH.jpeg\">\n",
    "\n",
    "The data for the above image may look something like this:\n",
    "```python\n",
    "image = [height, width, 3]\n",
    "bounding_boxes = {\n",
    "  \"classes\": [0], # 0 is an arbitrary class ID representing \"cat\"\n",
    "  \"boxes\": [[0.25, 0.4, .15, .1]]\n",
    "   # bounding box is in \"rel_xywh\" format\n",
    "   # so 0.25 represents the start of the bounding box 25% of\n",
    "   # the way across the image.\n",
    "   # The .15 represents that the width is 15% of the image width.\n",
    "}\n",
    "```\n",
    "\n",
    "Since the inception of [*You Only Look Once*](https://arxiv.org/abs/1506.02640)\n",
    "(aka YOLO),\n",
    "object detection has primarily solved using deep learning.\n",
    "Most deep learning architectures do this by cleverly framing the object detection\n",
    "problem as a combination of many small classification problems and\n",
    "many regression problems.\n",
    "\n",
    "More specifically, this is done by generating many anchor boxes of varying\n",
    "shapes and sizes across the input images and assigning them each a class label,\n",
    "as well as `x`, `y`, `width` and `height` offsets.\n",
    "The model is trained to predict the class labels of each box, as well as the\n",
    "`x`, `y`, `width`, and `height` offsets of each box that is predicted to be an\n",
    "object.\n",
    "\n",
    "**Visualization of some sample anchor boxes**:\n",
    "\n",
    "<img width=\"400\" src=\"https://i.imgur.com/cJIuiK9.jpg\">\n",
    "\n",
    "Objection detection is a technically complex problem but luckily we offer a\n",
    "bulletproof approach to getting great results.\n",
    "Let's do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Perform detections with a pretrained model\n",
    "\n",
    "![](https://storage.googleapis.com/keras-nlp/getting_started_guide/prof_keras_beginner.png)\n",
    "\n",
    "The highest level API in the KerasCV Object Detection API is the `keras_cv.models` API.\n",
    "This API includes fully pretrained object detection models, such as\n",
    "`keras_cv.models.RetinaNet`.\n",
    "\n",
    "Let's get started by constructing a RetinaNet pretrained on the `pascalvoc`\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "pretrained_model = keras_cv.models.RetinaNet.from_preset(\n",
    "    \"retinanet_resnet50_pascalvoc\", bounding_box_format=\"xywh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Notice the `bounding_box_format` argument?\n",
    "\n",
    "Recall in the section above, the format of bounding boxes:\n",
    "\n",
    "```\n",
    "bounding_boxes = {\n",
    "  \"classes\": [num_boxes],\n",
    "  \"boxes\": [num_boxes, 4]\n",
    "}\n",
    "```\n",
    "\n",
    "This argument describes *exactly* what format the values in the `\"boxes\"`\n",
    "field of the label dictionary take in your pipeline.\n",
    "For example, a box in `xywh` format with its top left corner at the coordinates\n",
    "(100, 100) with a width of 55 and a height of 70 would be represented by:\n",
    "```\n",
    "[100, 100, 55, 75]\n",
    "```\n",
    "\n",
    "or equivalently in `xyxy` format:\n",
    "\n",
    "```\n",
    "[100, 100, 155, 175]\n",
    "```\n",
    "\n",
    "While this may seem simple, it is a critical piece of the KerasCV object\n",
    "detection API!\n",
    "Every component that processes bounding boxes requires\n",
    "`bounding_box_format` argument.\n",
    "You can read more about\n",
    "KerasCV bounding box formats [in the API docs](https://keras.io/api/keras_cv/bounding_box/formats/).\n",
    "\n",
    "\n",
    "This is done because there is no one correct format for bounding boxes!\n",
    "Components in different pipelines expect different formats, and so by requiring\n",
    "them to be specified we ensure that our components remain readable, reusable,\n",
    "and clear.\n",
    "Box format conversion bugs are perhaps the most common bug surface in object\n",
    "detection pipelines - by requiring this parameter we mitigate against these\n",
    "bugs (especially when combining code from many sources).\n",
    "\n",
    "Next let's load an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "filepath = tf.keras.utils.get_file(origin=\"https://i.imgur.com/gCNcJJI.jpg\")\n",
    "image = keras.utils.load_img(filepath)\n",
    "image = np.array(image)\n",
    "\n",
    "visualization.plot_image_gallery(\n",
    "    [image],\n",
    "    value_range=(0, 255),\n",
    "    rows=1,\n",
    "    cols=1,\n",
    "    scale=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "To use the `RetinaNet` architecture with a ResNet50 backbone, you'll need to\n",
    "resize your image to a size that is divisible by 64.  This is to ensure\n",
    "compatibility with the number of downscaling operations done by the convolution\n",
    "layers in the ResNet.\n",
    "\n",
    "If the resize operation distorts\n",
    "the input's aspect ratio, the model will perform signficantly poorer.  For the\n",
    "pretrained `\"retinanet_resnet50_pascalvoc\"` preset we are using, the final\n",
    "`MeanAveragePrecision` on the `pascalvoc/2012` evaluation set drops to `0.15`\n",
    "from `0.38` when using a naive resizing operation.\n",
    "\n",
    "Additionally, if you crop to preserve the aspect ratio as you do in classification\n",
    "your model may entirely miss some bounding boxes.  As such, when running inference\n",
    "on an object detection model we recommend the use of padding to the desired size,\n",
    "while resizing the longest size to match the aspect ratio.\n",
    "\n",
    "KerasCV makes resizing properly easy; simply pass `pad_to_aspect_ratio=True` to\n",
    "a `keras_cv.layers.Resizing` layer.\n",
    "\n",
    "This can be implemented in one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inference_resizing = keras_cv.layers.Resizing(\n",
    "    640, 640, pad_to_aspect_ratio=True, bounding_box_format=\"xywh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "This can be used as our inference preprocessing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "image_batch = inference_resizing([image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "`keras_cv.visualization.plot_bounding_box_gallery()` supports a `class_mapping`\n",
    "parameter to highlight what class each box was assigned to.  Let's assemble a\n",
    "class mapping now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class_ids = [\n",
    "    \"Aeroplane\",\n",
    "    \"Bicycle\",\n",
    "    \"Bird\",\n",
    "    \"Boat\",\n",
    "    \"Bottle\",\n",
    "    \"Bus\",\n",
    "    \"Car\",\n",
    "    \"Cat\",\n",
    "    \"Chair\",\n",
    "    \"Cow\",\n",
    "    \"Dining Table\",\n",
    "    \"Dog\",\n",
    "    \"Horse\",\n",
    "    \"Motorbike\",\n",
    "    \"Person\",\n",
    "    \"Potted Plant\",\n",
    "    \"Sheep\",\n",
    "    \"Sofa\",\n",
    "    \"Train\",\n",
    "    \"Tvmonitor\",\n",
    "    \"Total\",\n",
    "]\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Just like any other `keras.Model` you can predict bounding boxes using the\n",
    "`model.predict()` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "y_pred = pretrained_model.predict(image_batch)\n",
    "# y_pred is a bounding box Tensor:\n",
    "# {\"classes\": ..., boxes\": ...}\n",
    "visualization.plot_bounding_box_gallery(\n",
    "    image_batch,\n",
    "    value_range=(0, 255),\n",
    "    rows=1,\n",
    "    cols=1,\n",
    "    y_pred=y_pred,\n",
    "    scale=5,\n",
    "    font_scale=0.7,\n",
    "    bounding_box_format=\"xywh\",\n",
    "    class_mapping=class_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In order to support easy this easy and intuitive inference workflow, KerasCV\n",
    "performs non-max suppression inside of the `RetinaNet` class.\n",
    "Non-max suppression is a traditional computing algorithm that solves the problem\n",
    "of a model detecting multiple boxes for the same object.\n",
    "\n",
    "Non-max suppression is a highly configurable algorithm, and in most cases you\n",
    "will want to customize the settings of your model's non-max\n",
    "suppression operation.\n",
    "This can be done by overriding to the `model.prediction_decoder` attribute.\n",
    "\n",
    "To show this concept off, lets temporarily disable non-max suppression on our\n",
    "RetinaNet.  This can be done by writing to the `prediction_decoder` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# The following NonMaxSuppression layer is equivalent to disabling the operation\n",
    "prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(\n",
    "    bounding_box_format=\"xywh\",\n",
    "    from_logits=True,\n",
    "    iou_threshold=1.0,\n",
    "    confidence_threshold=0.0,\n",
    ")\n",
    "pretrained_model.prediction_decoder = prediction_decoder\n",
    "\n",
    "y_pred = pretrained_model.predict(image_batch)\n",
    "visualization.plot_bounding_box_gallery(\n",
    "    image_batch,\n",
    "    value_range=(0, 255),\n",
    "    rows=1,\n",
    "    cols=1,\n",
    "    y_pred=y_pred,\n",
    "    scale=5,\n",
    "    font_scale=0.7,\n",
    "    bounding_box_format=\"xywh\",\n",
    "    class_mapping=class_mapping,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, lets re-configure `keras_cv.layers.MultiClassNonMaxSuppression` for our\n",
    "use case!\n",
    "In this case, we will tune the `iou_threshold` to `0.2`, and the\n",
    "`confidence_threshold` to `0.7`.\n",
    "\n",
    "Raising the `confidence_threshold` will cause the model to only output boxes\n",
    "that have a higher confidence score.  `iou_threshold` controls the threshold of\n",
    "IoU two boxes must have in order for one to be pruned out.\n",
    "[More information on these parameters may be found in the TensorFlow API docs](https://www.tensorflow.org/api_docs/python/tf/image/combined_non_max_suppression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(\n",
    "    bounding_box_format=\"xywh\",\n",
    "    from_logits=True,\n",
    "    # Decrease the required threshold to make predictions get pruned out\n",
    "    iou_threshold=0.2,\n",
    "    # Tune confidence threshold for predictions to pass NMS\n",
    "    confidence_threshold=0.7,\n",
    ")\n",
    "pretrained_model.prediction_decoder = prediction_decoder\n",
    "\n",
    "y_pred = pretrained_model.predict(image_batch)\n",
    "visualization.plot_bounding_box_gallery(\n",
    "    image_batch,\n",
    "    value_range=(0, 255),\n",
    "    rows=1,\n",
    "    cols=1,\n",
    "    y_pred=y_pred,\n",
    "    scale=5,\n",
    "    font_scale=0.7,\n",
    "    bounding_box_format=\"xywh\",\n",
    "    class_mapping=class_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "That looks a lot better!\n",
    "\n" ]
}
],
"metadata": {
"accelerator": "GPU",
"colab": {
 "collapsed_sections": [],
 "name": "object_detection_keras_cv",
 "private_outputs": false,
 "provenance": [],
 "toc_visible": true
},
"kernelspec": {
 "display_name": "Python 3",
 "language": "python",
 "name": "python3"
},
"language_info": {
 "codemirror_mode": {
  "name": "ipython",
  "version": 3
 },
 "file_extension": ".py",
 "mimetype": "text/x-python",
 "name": "python",
 "nbconvert_exporter": "python",
 "pygments_lexer": "ipython3",
 "version": "3.7.0"
}
},
"nbformat": 4,
"nbformat_minor": 0
}
